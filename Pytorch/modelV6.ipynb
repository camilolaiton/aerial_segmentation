{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8787f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10a0639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods\n",
    "\n",
    "\n",
    "def group_dict_by_key(cond, d):\n",
    "    return_val = [dict(), dict()]\n",
    "    for key in d.keys():\n",
    "        match = bool(cond(key))\n",
    "        ind = int(not match)\n",
    "        return_val[ind][key] = d[key]\n",
    "    return (*return_val, )\n",
    "\n",
    "\n",
    "def group_by_key_prefix_and_remove_prefix(prefix, d):\n",
    "    kwargs_with_prefix, kwargs = group_dict_by_key(\n",
    "        lambda x: x.startswith(prefix), d)\n",
    "    kwargs_without_prefix = dict(\n",
    "        map(lambda x: (x[0][len(prefix):], x[1]),\n",
    "            tuple(kwargs_with_prefix.items())))\n",
    "    return kwargs_without_prefix, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cff2c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):  # layernorm, but done in the channel dimension #1\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        std = torch.var(x, dim=1, unbiased=False, keepdim=True).sqrt()\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps) * self.g + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "977c2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim * mult, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(dim * mult, dim, 1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e9262f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthWiseConv2d(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, kernel_size, padding, stride, bias = True):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_in, kernel_size = kernel_size, padding = padding, groups = dim_in, stride = stride, bias = bias),\n",
    "            nn.BatchNorm2d(dim_in),\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size = 1, bias = bias)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf513cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, proj_kernel, kv_proj_stride, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        padding = proj_kernel // 2\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "\n",
    "        self.to_q = DepthWiseConv2d(dim, inner_dim, proj_kernel, padding = padding, stride = 1, bias = False)\n",
    "        self.to_kv = DepthWiseConv2d(dim, inner_dim * 2, proj_kernel, padding = padding, stride = kv_proj_stride, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(inner_dim, dim, 1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        b, n, _, y, h = *shape, self.heads\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = 1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h d) x y -> (b h) (x y) d', h = h), (q, k, v))\n",
    "\n",
    "        dots = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, y = y)\n",
    "        return self.to_out(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aa8cada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, proj_kernel, kv_proj_stride, depth, heads, dim_head = 64, mlp_mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, proj_kernel = proj_kernel, kv_proj_stride = kv_proj_stride, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_mult, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5ac3c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "071aece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, up_sample_mode):\n",
    "        super(UpBlock, self).__init__()\n",
    "        if up_sample_mode == 'conv_transpose':\n",
    "            #self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n",
    "            self.up_sample = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2)        \n",
    "        elif up_sample_mode == 'bilinear':\n",
    "            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    #def forward(self, down_input, skip_input= torch.Tensor().to(device)):\n",
    "    #    x = self.up_sample(down_input)\n",
    "    #    x = torch.cat([x, skip_input], dim=1)\n",
    "    #    return self.double_conv(x)\n",
    "    \n",
    "    def forward(self, down_input):        \n",
    "        x = self.up_sample(down_input)        \n",
    "        #x = torch.cat([x, skip_input], dim=1)\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UpBlockskip(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, up_sample_mode):\n",
    "        super(UpBlockskip, self).__init__()\n",
    "        if up_sample_mode == 'conv_transpose':\n",
    "            #self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n",
    "            self.up_sample = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2)        \n",
    "        elif up_sample_mode == 'bilinear':\n",
    "            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    #def forward(self, down_input, skip_input= torch.Tensor().to(device)):\n",
    "    #    x = self.up_sample(down_input)\n",
    "    #    x = torch.cat([x, skip_input], dim=1)\n",
    "    #    return self.double_conv(x)\n",
    "    \n",
    "    def forward(self, down_input, skip_input):        \n",
    "        x = torch.cat([down_input, skip_input], dim=1)\n",
    "        x = self.up_sample(x)          \n",
    "        return self.double_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2def6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CvT(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            num_classes=2, s1_emb_dim=32, s1_emb_kernel=5, s1_emb_stride=2,\n",
    "            s1_proj_kernel=3, s1_kv_proj_stride=2, s1_heads=1, s1_depth=1, s1_mlp_mult=4,\n",
    "    \n",
    "            s2_emb_dim=32, s2_emb_kernel=3, s2_emb_stride=2, s2_proj_kernel=3, s2_kv_proj_stride=2,\n",
    "            s2_heads=1, s2_depth=2, s2_mlp_mult=4,\n",
    "    \n",
    "            s3_emb_dim=32, s3_emb_kernel=3, s3_emb_stride=2, s3_proj_kernel=3,\n",
    "            s3_kv_proj_stride=2, s3_heads=6, s3_depth=10, s3_mlp_mult=4, dropout=0.,\n",
    "            up_sample_mode='bilinear'):\n",
    "        super().__init__()\n",
    "        kwargs = dict(locals())\n",
    "\n",
    "        dim_input = 3\n",
    "        layers = []\n",
    "\n",
    "        #Attention path\n",
    "        self.att_convB1 = nn.Conv2d(dim_input,\n",
    "                                    s1_emb_dim,\n",
    "                                    kernel_size=s1_emb_kernel,\n",
    "                                    padding=(s1_emb_kernel // 2),\n",
    "                                    stride=s1_emb_stride)\n",
    "        self.att_normB1 = LayerNorm(s1_emb_dim)\n",
    "        self.att_tranB1 = Transformer(dim=s1_emb_dim,\n",
    "                                      proj_kernel=s1_proj_kernel,\n",
    "                                      kv_proj_stride=s1_kv_proj_stride,\n",
    "                                      depth=s1_depth,\n",
    "                                      heads=s1_heads,\n",
    "                                      mlp_mult=s1_mlp_mult,\n",
    "                                      dropout=dropout)\n",
    "\n",
    "        self.att_convB2 = nn.Conv2d(s1_emb_dim,\n",
    "                                    s2_emb_dim,\n",
    "                                    kernel_size=s2_emb_kernel,\n",
    "                                    padding=(s2_emb_kernel // 2),\n",
    "                                    stride=s2_emb_stride)\n",
    "        self.att_normB2 = LayerNorm(s2_emb_dim)\n",
    "        self.att_tranB2 = Transformer(dim=s2_emb_dim,\n",
    "                                      proj_kernel=s2_proj_kernel,\n",
    "                                      kv_proj_stride=s2_kv_proj_stride,\n",
    "                                      depth=s2_depth,\n",
    "                                      heads=s2_heads,\n",
    "                                      mlp_mult=s2_mlp_mult,\n",
    "                                      dropout=dropout)\n",
    "\n",
    "        self.att_convB3 = nn.Conv2d(s2_emb_dim,\n",
    "                                    s3_emb_dim,\n",
    "                                    kernel_size=s3_emb_kernel,\n",
    "                                    padding=(s3_emb_kernel // 2),\n",
    "                                    stride=s3_emb_stride)\n",
    "        self.att_normB3 = LayerNorm(s3_emb_dim)\n",
    "        self.att_tranB3 = Transformer(dim=s3_emb_dim,\n",
    "                                      proj_kernel=s3_proj_kernel,\n",
    "                                      kv_proj_stride=s3_kv_proj_stride,\n",
    "                                      depth=s3_depth,\n",
    "                                      heads=s3_heads,\n",
    "                                      mlp_mult=s3_mlp_mult,\n",
    "                                      dropout=dropout)\n",
    "\n",
    "        self.double_conv = DoubleConv(s3_emb_dim, 512)\n",
    "\n",
    "        #Upsampling path\n",
    "        self.up_convB4 = UpBlock(512, 128, up_sample_mode)\n",
    "        self.up_convB3 = UpBlockskip(128+32, 64, up_sample_mode)\n",
    "        self.norm1     = LayerNorm(64)\n",
    "        self.up_convB2 = UpBlockskip(64+32, 32, up_sample_mode)\n",
    "        self.norm2     = LayerNorm(32)\n",
    "        #self.up_convB1 = UpBlock(32, 16, up_sample_mode)\n",
    "\n",
    "        # Output match\n",
    "        self.conv_last = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.att_convB1(x) ;skip_l1 = x #;print('B1',x.shape)        \n",
    "        x = self.att_normB1(x) #;print('B1',x.shape)\n",
    "        x = self.att_tranB1(x) #;print('B1',x.shape)\n",
    "        \n",
    "        x = self.att_convB2(x) #;print('B2',x.shape)\n",
    "        skip_l2 = x            #;print('B2',x.shape)\n",
    "        x = self.att_normB2(x) #;print('B2',x.shape)\n",
    "        x = self.att_tranB2(x) #;print('B2',x.shape)\n",
    "        \n",
    "        x = self.att_convB3(x) #;print('B3',x.shape)\n",
    "        x = self.att_normB3(x) #;print('B3',x.shape)\n",
    "        x = self.att_tranB3(x) #;print('B3',x.shape)\n",
    "\n",
    "        x = self.double_conv(x) #;print('Dob',x.shape)\n",
    "        \n",
    "        x = self.up_convB4(x) #;print(x.shape)        \n",
    "        x = self.up_convB3(x, skip_l2)        #;print(x.shape)\n",
    "        x = self.norm1(x)\n",
    "        x = self.up_convB2(x, skip_l1) #;print(x.shape)\n",
    "        x = self.norm2(x)\n",
    "       # print('x:',x.shape, ' - skip:', skip_l1.shape)\n",
    "        #x = self.up_convB1(x)\n",
    "        x = self.conv_last(x) #;print(x.shape)\n",
    "        #print('x:',x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5e17db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape     Param #\n",
       "============================================================================================\n",
       "CvT                                                          --               --\n",
       "├─Transformer: 1                                             --               --\n",
       "│    └─ModuleList: 2-1                                       --               --\n",
       "│    │    └─ModuleList: 3-1                                  --               17,408\n",
       "├─Transformer: 1                                             --               --\n",
       "│    └─ModuleList: 2-2                                       --               --\n",
       "│    │    └─ModuleList: 3-2                                  --               33,792\n",
       "│    │    └─ModuleList: 3-3                                  --               33,792\n",
       "├─Transformer: 1                                             --               --\n",
       "│    └─ModuleList: 2-3                                       --               --\n",
       "│    │    └─ModuleList: 3-4                                  --               41,984\n",
       "│    │    └─ModuleList: 3-5                                  --               41,984\n",
       "│    │    └─ModuleList: 3-6                                  --               41,984\n",
       "│    │    └─ModuleList: 3-7                                  --               41,984\n",
       "│    │    └─ModuleList: 3-8                                  --               41,984\n",
       "│    │    └─ModuleList: 3-9                                  --               41,984\n",
       "│    │    └─ModuleList: 3-10                                 --               41,984\n",
       "│    │    └─ModuleList: 3-11                                 --               41,984\n",
       "│    │    └─ModuleList: 3-12                                 --               41,984\n",
       "│    │    └─ModuleList: 3-13                                 --               41,984\n",
       "├─Conv2d: 1-1                                                [1, 32, 128, 128] 2,432\n",
       "├─LayerNorm: 1-2                                             [1, 32, 128, 128] 64\n",
       "├─Transformer: 1-3                                           [1, 32, 128, 128] --\n",
       "├─Conv2d: 1-4                                                [1, 32, 64, 64]  9,248\n",
       "├─LayerNorm: 1-5                                             [1, 32, 64, 64]  64\n",
       "├─Transformer: 1-6                                           [1, 32, 64, 64]  --\n",
       "├─Conv2d: 1-7                                                [1, 32, 32, 32]  9,248\n",
       "├─LayerNorm: 1-8                                             [1, 32, 32, 32]  64\n",
       "├─Transformer: 1-9                                           [1, 32, 32, 32]  --\n",
       "├─DoubleConv: 1-10                                           [1, 512, 32, 32] --\n",
       "│    └─Sequential: 2-4                                       [1, 512, 32, 32] --\n",
       "│    │    └─Conv2d: 3-14                                     [1, 512, 32, 32] 147,968\n",
       "│    │    └─BatchNorm2d: 3-15                                [1, 512, 32, 32] 1,024\n",
       "│    │    └─ReLU: 3-16                                       [1, 512, 32, 32] --\n",
       "│    │    └─Conv2d: 3-17                                     [1, 512, 32, 32] 2,359,808\n",
       "│    │    └─BatchNorm2d: 3-18                                [1, 512, 32, 32] 1,024\n",
       "│    │    └─ReLU: 3-19                                       [1, 512, 32, 32] --\n",
       "├─UpBlock: 1-11                                              [1, 128, 64, 64] --\n",
       "│    └─Upsample: 2-5                                         [1, 512, 64, 64] --\n",
       "│    └─DoubleConv: 2-6                                       [1, 128, 64, 64] --\n",
       "│    │    └─Sequential: 3-20                                 [1, 128, 64, 64] 738,048\n",
       "├─UpBlockskip: 1-12                                          [1, 64, 128, 128] --\n",
       "│    └─Upsample: 2-7                                         [1, 160, 128, 128] --\n",
       "│    └─DoubleConv: 2-8                                       [1, 64, 128, 128] --\n",
       "│    │    └─Sequential: 3-21                                 [1, 64, 128, 128] 129,408\n",
       "├─LayerNorm: 1-13                                            [1, 64, 128, 128] 128\n",
       "├─UpBlockskip: 1-14                                          [1, 32, 256, 256] --\n",
       "│    └─Upsample: 2-9                                         [1, 96, 256, 256] --\n",
       "│    └─DoubleConv: 2-10                                      [1, 32, 256, 256] --\n",
       "│    │    └─Sequential: 3-22                                 [1, 32, 256, 256] 37,056\n",
       "├─LayerNorm: 1-15                                            [1, 32, 256, 256] 64\n",
       "├─Conv2d: 1-16                                               [1, 2, 256, 256] 66\n",
       "============================================================================================\n",
       "Total params: 3,940,546\n",
       "Trainable params: 3,940,546\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 10.94\n",
       "============================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 327.94\n",
       "Params size (MB): 15.76\n",
       "Estimated Total Size (MB): 344.49\n",
       "============================================================================================"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = CvT()\n",
    "\n",
    "#summary(model.to('cpu'), (1, 3, 256, 256), col_width=16, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca1f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db1f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e03db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int,long)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(true, logits, weights, ignore=255):\n",
    "    \"\"\"Computes the weighted multi-class cross-entropy loss.\n",
    "    Args:\n",
    "        true: a tensor of shape [B, 1, H, W].\n",
    "        logits: a tensor of shape [B, C, H, W]. Corresponds to\n",
    "            the raw output or logits of the model.\n",
    "        weight: a tensor of shape [C,]. The weights attributed\n",
    "            to each class.\n",
    "        ignore: the class index to ignore.\n",
    "    Returns:\n",
    "        ce_loss: the weighted multi-class cross-entropy loss.\n",
    "    \"\"\"\n",
    "    ce_loss = F.cross_entropy(\n",
    "        logits.float(),\n",
    "        true.long(),\n",
    "        ignore_index=ignore,\n",
    "        weight=weights,\n",
    "    )\n",
    "    return ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c14add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        self.__name__ = 'DiceBCELoss'\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss = 1 - (2. * intersection + smooth) / (inputs.sum() +\n",
    "                                                        targets.sum() + smooth)\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "\n",
    "        return Dice_BCE\n",
    "    \n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.__name__ = 'IoULoss'\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs)       \n",
    "        inputs = torch.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #intersection is equivalent to True Positive count\n",
    "        #union is the mutually inclusive area of all labels & predictions \n",
    "        \n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection \n",
    "        \n",
    "        IoU = (intersection + smooth)/(union + smooth)\n",
    "                \n",
    "        return 1 - IoU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efea048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WCrossEntropy(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, DEVICE='cuda'):\n",
    "        super(WCrossEntropy, self).__init__()\n",
    "        self.__name__ = 'WCEntropy'\n",
    "        self.weight = None\n",
    "        if weight!= None:\n",
    "            self.weight = torch.tensor(weight).to(DEVICE)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # From tensor [B, C, H, W] to tensor [B, H, W]        \n",
    "        targets = torch.argmax(targets, dim = 1)\n",
    "        loss = nn.CrossEntropyLoss(weight=self.weight)\n",
    "        output = loss(inputs, targets )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bcf515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weigths = [2.0, 8.0]\n",
    "# inputs = torch.FloatTensor(2, 2, 5, 5)\n",
    "# logits = torch.FloatTensor(2, 2, 5, 5)\n",
    "# weigths = torch.FloatTensor(class_weigths)\n",
    "\n",
    "# print('inputs:', inputs.shape)\n",
    "# print('weigths', weigths.shape)\n",
    "# print('logits ', logits.shape)\n",
    "\n",
    "# logits = torch.tensor(logits, dtype=torch.long)\n",
    "# #logits = logits.long()\n",
    "# #logits\n",
    "# #inputs = inputs.view(-1)\n",
    "# #print('inputs:', inputs.shape)\n",
    "\n",
    "# # loss = nn.CrossEntropyLoss()\n",
    "# # loss(inputs, logits)\n",
    "\n",
    "# print(logits[0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e437e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot = torch.tensor(\n",
    "#                     np.array([\n",
    "#                     [[[1, 1, 1, 0, 0], [0, 0, 0, 0, 0]],    \n",
    "#                     [[0, 0, 0, 0, 0], [1, 1, 1, 0, 0]],\n",
    "#                     [[0, 0, 0, 1, 1], [0, 0, 0, 1, 1]],],\n",
    "#                     [[[1, 1, 1, 0, 0], [0, 0, 0, 0, 0]],    \n",
    "#                     [[0, 0, 0, 0, 0], [1, 1, 1, 0, 0]],\n",
    "#                     [[0, 0, 0, 1, 1], [0, 0, 0, 1, 1]],]\n",
    "#                    ])\n",
    "# )\n",
    "# print(one_hot.shape)\n",
    "# print(one_hot)\n",
    "\n",
    "# x = torch.argmax(one_hot, dim = 1)\n",
    "\n",
    "# print(x.shape)\n",
    "# print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, cv2\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def one_hot_encode(label, label_values):\n",
    "#     semantic_map = []\n",
    "#     for colour in label_values:\n",
    "#         equality = np.equal(label, colour)\n",
    "#         class_map = np.all(equality, axis = -1)\n",
    "#         semantic_map.append(class_map)\n",
    "#     semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "#     return semantic_map\n",
    "\n",
    "\n",
    "# DATA_DIR = './datasets/tiff/'\n",
    "# masks_dir = os.path.join(DATA_DIR, 'train_labels')\n",
    "# class_dict = pd.read_csv(\"./datasets/label_class_dict.csv\")\n",
    "# class_names = class_dict['name'].tolist()\n",
    "# class_rgb_values = class_dict[['r','g','b']].values.tolist()\n",
    "\n",
    "# select_classes = ['background', 'building']\n",
    "# select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n",
    "# select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
    "\n",
    "# mask_paths  = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n",
    "# freq = np.zeros((1,2))\n",
    "# for img in mask_paths:\n",
    "#     print(img)\n",
    "#     mask  = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)   \n",
    "#     mask = one_hot_encode(mask, class_rgb_values).astype('float')\n",
    "#     mask = np.argmax(mask , axis = -1)\n",
    "#     #print(mask)\n",
    "#     #plt.imshow(mask, cmap='gray')\n",
    "#     unique, counts = np.unique(mask, return_counts=True)    \n",
    "#     freq += counts\n",
    "# print(freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# select_classes = ['background', 'building']\n",
    "# freqs = freq[0]  #Error in prior cell, line 36\n",
    "# total_pixels = np.sum(freqs)\n",
    "# freq_prop = freqs/total_pixels\n",
    "\n",
    "\n",
    "# class_p = {select_classes[0]:freq_prop[0], select_classes[1]:freq_prop[1]}\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# ax = sns.barplot(x=list(class_p.keys()), y=list(class_p.values()))\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# print(f'total_pixels:{total_pixels:.0f}')\n",
    "# print(f'{select_classes[0]}:{freqs[0]:.0f}({freq_prop[0]*100:.1f}%) {select_classes[1]}:{freqs[1]:.0f}({freq_prop[1]*100:.1f}%)')\n",
    "# ws  = total_pixels / (len(select_classes) * freqs)\n",
    "# wsn = ws / np.linalg.norm(ws)\n",
    "# print(f'weigth_{select_classes[0]}:{ws[0]:.4f} weigth_{select_classes[1]}:{ws[1]:.4f}')\n",
    "# print(f'Nweigth_{select_classes[0]}:{wsn[0]:.4f} Nweigth_{select_classes[1]}:{wsn[1]:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
